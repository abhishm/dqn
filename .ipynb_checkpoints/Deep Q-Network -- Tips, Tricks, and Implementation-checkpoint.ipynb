{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: Deep Q-Network -- Tips, Tricks, and Implementation\n",
    "author: \"Abhishek Mishra\"\n",
    "---\n",
    "<script src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script>\n",
    "<style>\n",
    ".center-image\n",
    "{\n",
    "    margin: 0 auto;\n",
    "    display: block;\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Q-learning is one of the fundamental method of solving a reinforcement learning problem. In reinforcement learning problem, there is an agent that observes the present state of the environment, takes an action, receives a reward and the environment goes to a next state. This process is repeated until some terminating criterion is met. The batch of state, action, and reward forms one trajectory of the environment. The goal of the agent is to maximize its total reward obtained in one trajectory. The following figure represents an archetypical setting of a reinforcement learning problem: \n",
    "\n",
    "![rl](figures/rl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning solves a reinforcement learning problem by giving us the measure of `goodness` of actions at a given state. Assume that we are solving a reinforcement learning problem and the environment presents us the observation $s'$. We can choose one of the two actions $a_1$ or $a_2$ at the presented state $s'$. Assume that a fairy has given us access to a function $Q(s, a)$ that can give us a measure of goodness of all actions $a$ at any given state $s$. We can check whether $Q(s', a_1) > Q(s', a_2)$. If it is true, we will take action $a_1$ and state $s'$ otherwise we will take action $a_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q-learning is all about finding the function $Q(s, a)$ thats why it is called Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the function $Q(s, a)?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how to find the function $Q(s, a)$, we need to know what does $Q(s, a)$ stands for. $Q(s, a)$ represent the maximum reward that we can get in the reinforcement learning problem after taking action $a$ at state $s$ until the end of the episode. The following figure represent a portion of a trajectory in a reinforcement learning setting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://cs.stanford.edu/people/karpathy/reinforcejs/img/sarsa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure tells us that we are at state $s_t$, we take action $a_t$ and we reach to a new state $s_{t + 1}$. As we know $Q(s_t, a_t)$ represents the highest reward that we can get at state $s_t$ after taking action $a_t$. From the above figure, we see that after taking actions, $a_t$ at state $s_t$ we receive a reward $r_t$ and we reach to a new state $s_{t + 1}$. Note that the maximum reward that we can get at state $s_{t + 1}$ is $\\max_{a_{t + 1}} Q(s_{t + 1}, a_{t + 1})$. Hence $Q(s_t, a_t)$ must satifies the following equations:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = r_t + \\max_{a_{t + 1}} Q(s_{t + 1}, a_{t + 1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick to obtain $Q(s, a)$ is to define $Q(s, a)$ arbitrary for all state and action pairs initially and then iteratively change $Q(s, a)$ such that it satisfies or come close to satisfying the above equation for all states, actions pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is important to note here that it does not matter what policy is used to generate the trajectories. The policy that is used to generate trajectories is generally referred as behavior policy. The policy for which we update the state-action pair is called the target policy. Q-learning collects the trajectories from some behavior policy and update the Q-values for the optimal policy. Since the beahvior and target policy are not the same for Q-learning, Q-leanring is an off-policy method. Although, Q-learning is an off-policy methotd, it makes sure that as long as the behavior policy is sufficiently exploratory i.e. it ensures that all the state, action pairs would have non-zero probability of occuring, Q-learning would converge to the optimal policy for a finite state-action reinforcement learning problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A classical example from grid world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem of continuous state-space and the curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous state-space makes Q-learning harder for following two reasons:\n",
    "1. We collect trajectories from the environment and based on the states that we visit in the trajectory, we update $Q(s, a)$ for them. It is impossible to visit all the states when the states are continuous hence it is not feasible to get a good estimate of $Q(s, a)$. \n",
    "2. Even if we decide to discretize the state-space, if the state-space is high dimensional, we need a lot of trajectories to find a good estimate of $Q(s, a)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Solution for continuous state-space\n",
    "\n",
    "The solution for the continuous state-space problem is to use `function approximation`. The function approximation is a fancy way to say that we have a function $f$, that is parametrized by $w$ that will give us an approximation of true action-values $Q(s, a)$. For example, we can use a neural network as following to approximate the action-values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cr](figures/critic_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize parameters of neural networks ($w$) arbitrary and we can keep changing the $w$ such that we minimize the following error\n",
    "$$\n",
    "w = \\arg\\!\\min\\left(f^w(s, a) - \\left(r + \\max_b(f^w(s', b)\\right) \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical instability of Q-learning with function approximation\n",
    "\n",
    "Q-learning is a bootstrapping method. By bootstrapping, we mean the updating the action-values estimate on the basis of other action-values estimate. Off-policy bootstrap methods are known to diverge and can potentially give infinite Mean Square Error (MSE) on the optimization objective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) to rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN approach is introduced to make Q-learning stable mainly for the high dimensional problems. DQN approach inorporates several techniques to make Q-learning stable that are enumerated as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning with function approximation solves a regression problem where it moved the network parameters such that it reduces the TD-error. This is a supervised learning problem and supervised learning algorithms do good when the data comes from an independent-identical distribution (IID). But in a reinforcement learning problem, the states, actions, rewards tuples are sequential in nature. It is not a good idea to reduce the MSE on this sequential data. Moreover, in Q-learning, once we used a data point to update the models, we don't reuse it. Replay buffer is introduced to tackle these two problems. The idea behind replay buffer is simple and very effective. Replay buffer stores the interactions from the environment and select random data points from the history and use them for updating the Q-network parameter. We implemented a class for replay buffer. This class provides methods for adding the new experience to the buffer and sampling the random experience from the buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_items = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def add(self, item):\n",
    "        if self.num_items < self.buffer_size:\n",
    "            self.buffer.append(item)\n",
    "            self.num_items += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(item)\n",
    "\n",
    "    def add_items(self, items):\n",
    "        for item in items:\n",
    "            self.add(item)\n",
    "\n",
    "    def add_batch(self, batch):\n",
    "        keys = [\"states\", \"actions\", \"rewards\", \"next_states\", \"dones\"]\n",
    "        items = []\n",
    "        for i in range(len(batch[\"states\"])):\n",
    "            item = []\n",
    "            for key in keys:\n",
    "                item.append(batch[key][i])\n",
    "            items.append(item)\n",
    "        self.add_items(items)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        keys = [\"states\", \"actions\", \"rewards\", \"next_states\", \"dones\"]\n",
    "        samples = self.sample(batch_size)\n",
    "        samples = zip(*samples)\n",
    "        batch = {key: np.array(value) for key, value in zip(keys, samples)}\n",
    "        return batch\n",
    "\n",
    "    def count(self):\n",
    "        return self.num_items\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Target Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, Q-learning is a bootstrapping method and we change the action-values estimate of present state-actions based on the action-value estimates of the next states-actions. Since our estimate for action-values changes continuously, the target changes continously.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bounded error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
